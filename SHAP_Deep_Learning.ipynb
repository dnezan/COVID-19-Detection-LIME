{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SHAP_Deep_Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ISK1XQkvltV",
        "colab_type": "text"
      },
      "source": [
        "#**SHAP Approach to Deep Learning**\n",
        "#The Need for SHAP\n",
        "  In this day and age of increasingly prevalent solutions to modern day problems using Artificial Intelligence, it is important to be able to evaluate and interpret the complex models that make these decisions. The more complex a real world problem is, the more complex and incoherent a neural network can become. For example, the neural network for a simple classification problem is much more minimal when compared to a deep learning neural network that is used to predict airline delays due to weather and a plethora of other factors. Therefore arises a need for a system that helps understand and evaluate the complex neural networks in order to leverage the solutions in more meaningful ways and in order to futher improve the accuracy of the model.<br><br>\n",
        "  The SHapley Additive exPlanations, or SHAP explanations is an efficient alternative to the LIME explanations that we used in Part 1 of the COVID 19 project.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvQgfaki0X8g",
        "colab_type": "text"
      },
      "source": [
        "#What is SHAP?\n",
        "SHAP is a unified framework that is used to assign each feature its very own \"importance\" value; this value depends on how much the particular feature contributes to the overall output of the model. The SHAPly values for each feature are calculated separately by comparing the model performance using every permutation of the avaliable features, and performing operations until the individual SHAPly value of each feature is found. For example if we have 3 players or features in the system, we calculate the reward per feature by considering the permutations of 1 player, 2 players, and all 3 players. At the ebd, we arrive at the reward per feature based on the Shaply values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvgZaFqI5ZbG",
        "colab_type": "text"
      },
      "source": [
        "#SHAP Explainability in COVID-19 Project\n",
        "Previously in Part 1 of the COVID-19 project, we used ResNet deep learning classifier kernel and implemeted LIME explainability in order to evaluate the output produced by the neural network. LIME works in accordance to the local linear approximation of a modelâ€™s behaviour. This means that even though the data may be globally very complex and intricate, it is easier to deal with the local vicinity of certain areas of interest. In this model, the weights are assigned by proximity in order to come up with a solution to the classification problem.<br><br>\n",
        "An alternative framework to LIME is SHAP, whose main advantage is that it is an ensemble or unification of several frameworks. What makes SHAP unique is that it is able to identify a new class of additive feature importance measures, theoretical results which show that there is exists a unique solution in this class with a set of desirable properties. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixi2YuMhFXlR",
        "colab_type": "text"
      },
      "source": [
        "#The Difference Between LIME and SHAP\n",
        "LIME explanations are built based on the local information around a particular point of interest. What makes SHAP much more powerful is that it is able to come to the conclusion based on a chosen baseline, while also considering the overall contribution of the particular feature vector in forming the model.\n"
      ]
    }
  ]
}